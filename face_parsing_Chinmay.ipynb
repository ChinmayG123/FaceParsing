{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"RJFK2Vl55wT6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690656081637,"user_tz":420,"elapsed":287,"user":{"displayName":"Chinmay Gowdru","userId":"11071115537793324064"}},"outputId":"ec01d2b5-27c5-4851-e60f-65e72703748b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["\n","!pip install git+https://github.com/FacePerceiver/facer.git@main\n","!pip install timm\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from IPython.display import Image, display\n","\n","import sys\n","import torch\n","\n","import torch.nn.functional as F\n","sys.path.append('..')\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","source":["!apt-get install -y git\n","!git config --global user.name \"ChinmayG123\"\n","!git config --global user.email \"chinmaygowdru2@gmail.com\"\n"],"metadata":{"id":"rFnjo_iV8M7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/FacePerceiver/facer.git"],"metadata":{"id":"TubEb59W8hdB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download('/content/drive/MyDrive/face_parsing_Chinmay.ipynb')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"7B8A72wv8m-7","executionInfo":{"status":"ok","timestamp":1690656059545,"user_tz":420,"elapsed":5,"user":{"displayName":"Chinmay Gowdru","userId":"11071115537793324064"}},"outputId":"79f50010-b6c9-4180-be4e-aaa19c3680a2"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_5d557cf5-9925-485c-9f32-3ecaeedbee8c\", \"face_parsing.ipynb\", 37919)"]},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fXmac6WB5wT8","executionInfo":{"status":"ok","timestamp":1690331318341,"user_tz":420,"elapsed":1670,"user":{"displayName":"Chinmay Gowdru","userId":"11071115537793324064"}},"outputId":"bc5c11e2-4bf6-4ee6-e6c1-2599b0999e3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Package(s) not found: facer\u001b[0m\u001b[33m\n","\u001b[0mNo faces detected.\n"]}],"source":["import facer\n","!pip show facer\n","from PIL import Image as PILImage\n","\n","import os\n","import cv2\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","\n","image = facer.hwc2bchw(facer.read_hwc('drive/MyDrive/downsampledimages/12.jpg')\n","                       ).to(device=device)  # image: 1 x 3 x h x w\n","\n","face_detector = facer.face_detector('retinaface/mobilenet', device=device)\n","with torch.inference_mode():\n","    faces = face_detector(image)\n","\n","# Check if faces is empty\n","if len(faces) == 0:\n","    print(\"No faces detected.\")\n","else:\n","    facer.show_bchw(facer.draw_bchw(image, faces))\n","\n","#facer.show_bchw(facer.draw_bchw(image, faces))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-g2-CB6D5wT-"},"outputs":[],"source":["face_parser = facer.face_parser('farl/celebm/448', device=device) # optional \"farl/lapa/448\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaGUHuy85wT_"},"outputs":[],"source":["with torch.inference_mode():\n","    faces = face_parser(image, faces)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeNejeGK5wT_"},"outputs":[],"source":["import pandas as pd\n","\n","\n","seg_logits = faces['seg']['logits']\n","\n","seg_probs = seg_logits.softmax(dim=1)  # nfaces x nclasses x h x w\n","\n","# Get the segmentation labels (class index) for each pixel\n","segmentation_labels = seg_probs.argmax(dim=1)\n","\n","# Optionally, if you need to keep the same dimensions as the `seg_probs` tensor:\n","segmentation_labels = segmentation_labels.unsqueeze(1)\n","\n","# Now you have the segmentation labels for each pixel, and you can use them for further processing or visualization.\n","# print(\"Segmentation Labels\", segmentation_labels)\n","\n","# Assuming `segmentation_labels` is the tensor obtained from the previous code\n","segmentation_labels_np = segmentation_labels.squeeze().cpu().numpy()\n","\n","# Save the segmentation_labels_np as an npy file\n","output_file = '/content/drive/MyDrive/segmentation_masks_npy/59.npy'\n","np.save(output_file, segmentation_labels_np)\n","\n","\n","\n","\n","# Select the first channel and convert it to a NumPy array\n","segmentation_labels_np = segmentation_labels[0].squeeze().cpu().numpy()\n","\n","# Compute the number of classes directly from the tensor\n","n_classes = segmentation_labels.max().item() + 1\n","\n","# Choose a suitable colormap for segmentation tasks (e.g., 'viridis' or 'jet')\n","cmap = plt.get_cmap('viridis', n_classes)  # You can adjust the colormap as desired\n","\n","# Create a new figure without displaying the plot\n","plt.figure(figsize=(10, 10))  # You can adjust the figure size as needed\n","\n","# Plot the segmentation labels as an image using the custom colormap\n","plt.imshow(segmentation_labels_np, cmap=cmap, vmin=0, vmax=n_classes - 1)\n","plt.axis('off')  # Turn off axis labels and ticks\n","plt.colorbar().remove()  # Remove the colorbar\n","\n","# Save the image directly using the imsave function\n","output_path = '/content/drive/MyDrive/segmentation_masks/59.png'\n","plt.imsave(output_path, segmentation_labels_np, cmap=cmap)\n","\n","# Close the figure to free up resources (optional)\n","plt.close()\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# # Set the target size to 2592 x 1728\n","# target_h, target_w = 1728, 2592\n","# # Upsample the segmentation_labels tensor to match the target size\n","\n","# upsampled_segmentation_labels = F.interpolate(segmentation_labels.float(), size=(target_h, target_w), mode='nearest').long()\n","\n","# # Select the first channel and convert it to a NumPy array\n","# upsampled_segmentation_labels_np = upsampled_segmentation_labels[0].squeeze().cpu().numpy()\n","\n","# # Compute the number of classes directly from the tensor\n","# n_classes = upsampled_segmentation_labels.max().item() + 1\n","\n","# # Choose a suitable colormap for segmentation tasks (e.g., 'viridis' or 'jet')\n","# cmap = plt.get_cmap('viridis', n_classes)  # You can adjust the colormap as desired\n","\n","# # Create a new figure without displaying the plot\n","# plt.figure(figsize=(10, 10))  # You can adjust the figure size as needed\n","\n","# # Plot the segmentation labels as an image using the custom colormap\n","# plt.imshow(upsampled_segmentation_labels_np, cmap=cmap, vmin=0, vmax=n_classes - 1)\n","# plt.axis('off')  # Turn off axis labels and ticks\n","# plt.colorbar().remove()  # Remove the colorbar\n","\n","# # Save the image directly using the imsave function\n","# output_path = '/content/drive/MyDrive/segmentation_masks_upsampled/3.png'\n","# plt.imsave(output_path, upsampled_segmentation_labels_np, cmap=cmap)\n","\n","# # Close the figure to free up resources (optional)\n","# plt.close()\n","\n","# # Reset the print options to the default\n","# torch.set_printoptions(profile='default')\n","\n","\n","\n","\n","\n","\n","\n","n_classes = seg_probs.size(1)\n","vis_seg_probs = seg_probs.argmax(dim=1).float()/n_classes*255\n","vis_img = vis_seg_probs.sum(0, keepdim=True)\n","facer.show_bhw(vis_img)\n","facer.show_bchw(facer.draw_bchw(image, faces))"]},{"cell_type":"code","source":["!pip install git+https://github.com/FacePerceiver/facer.git@main\n","!pip install timm\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from IPython.display import Image, display\n","\n","import sys\n","import torch\n","\n","import torch.nn.functional as F\n","sys.path.append('..')\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"iDY_xOjv_mBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# run on a directory of images\n","import facer\n","!pip show facer\n","from PIL import Image as PILImage\n","\n","import os\n","import cv2\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","\n","\n","# Define the path to the folder containing the images\n","image_folder_path = '/content/drive/MyDrive/512/11/'\n","\n","# Load all image filenames from the folder\n","image_filenames = os.listdir(image_folder_path)\n","\n","# Face Detector\n","face_detector = facer.face_detector('retinaface/mobilenet', device=device)\n","\n","# Face Parser\n","face_parser = facer.face_parser('farl/celebm/448', device=device) # optional \"farl/lapa/448\"\n","\n","\n","\n","# Loop through each image and perform face detection and parsing\n","for filename in image_filenames:\n","    image_path = os.path.join(image_folder_path, filename)\n","    image = facer.hwc2bchw(facer.read_hwc(image_path)).to(device=device)  # image: 1 x 3 x h x w\n","\n","    with torch.inference_mode():\n","        faces = face_detector(image)\n","\n","    if len(faces) == 0:\n","        print(f\"No faces detected in {filename}.\")\n","    else:\n","        with torch.inference_mode():\n","            faces = face_parser(image, faces)\n","\n","        seg_logits = faces['seg']['logits']\n","        seg_probs = seg_logits.softmax(dim=1)  # nfaces x nclasses x h x w\n","\n","        # Get the segmentation labels (class index) for each pixel\n","        segmentation_labels = seg_probs.argmax(dim=1)\n","\n","        # Optionally, if you need to keep the same dimensions as the `seg_probs` tensor:\n","        segmentation_labels = segmentation_labels.unsqueeze(1)\n","\n","        # Now you have the segmentation labels for each pixel, and you can use them for further processing or visualization.\n","        # print(\"Segmentation Labels\", segmentation_labels)\n","\n","        # Assuming `segmentation_labels` is the tensor obtained from the previous code\n","        segmentation_labels_np = segmentation_labels.squeeze().cpu().numpy()\n","\n","        # Save the segmentation_labels_np as an npy file\n","        output_file = os.path.join('/content/drive/MyDrive/512segmentation/11/', f'{filename}.npy')\n","        np.save(output_file, segmentation_labels_np)\n","\n","\n","\n","\n","\n","\n","\n","\n","# image = facer.hwc2bchw(facer.read_hwc('drive/MyDrive/512/1/11.jpg')\n","#                        ).to(device=device)  # image: 1 x 3 x h x w\n","\n","# face_detector = facer.face_detector('retinaface/mobilenet', device=device)\n","# with torch.inference_mode():\n","#     faces = face_detector(image)\n","\n","# # Check if faces is empty\n","# if len(faces) == 0:\n","#     print(\"No faces detected.\")\n","# else:\n","#     facer.show_bchw(facer.draw_bchw(image, faces))\n","\n","# #facer.show_bchw(facer.draw_bchw(image, faces))\n","\n","# face_parser = facer.face_parser('farl/celebm/448', device=device) # optional \"farl/lapa/448\"\n","\n","# with torch.inference_mode():\n","#     faces = face_parser(image, faces)\n","\n","# import pandas as pd\n","\n","\n","# seg_logits = faces['seg']['logits']\n","\n","# seg_probs = seg_logits.softmax(dim=1)  # nfaces x nclasses x h x w\n","\n","# # Get the segmentation labels (class index) for each pixel\n","# segmentation_labels = seg_probs.argmax(dim=1)\n","\n","# # Optionally, if you need to keep the same dimensions as the `seg_probs` tensor:\n","# segmentation_labels = segmentation_labels.unsqueeze(1)\n","\n","# # Now you have the segmentation labels for each pixel, and you can use them for further processing or visualization.\n","# # print(\"Segmentation Labels\", segmentation_labels)\n","\n","# # Assuming `segmentation_labels` is the tensor obtained from the previous code\n","# segmentation_labels_np = segmentation_labels.squeeze().cpu().numpy()\n","\n","# # Save the segmentation_labels_np as an npy file\n","# output_file = '/content/drive/MyDrive/512/1/11.jpg.npy'\n","# np.save(output_file, segmentation_labels_np)\n","\n","\n","\n","\n","# # Select the first channel and convert it to a NumPy array\n","# segmentation_labels_np = segmentation_labels[0].squeeze().cpu().numpy()\n","\n","# # Compute the number of classes directly from the tensor\n","# n_classes = segmentation_labels.max().item() + 1\n","\n","# # Choose a suitable colormap for segmentation tasks (e.g., 'viridis' or 'jet')\n","# cmap = plt.get_cmap('viridis', n_classes)  # You can adjust the colormap as desired\n","\n","# # Create a new figure without displaying the plot\n","# plt.figure(figsize=(10, 10))  # You can adjust the figure size as needed\n","\n","# # Plot the segmentation labels as an image using the custom colormap\n","# plt.imshow(segmentation_labels_np, cmap=cmap, vmin=0, vmax=n_classes - 1)\n","# plt.axis('off')  # Turn off axis labels and ticks\n","# plt.colorbar().remove()  # Remove the colorbar\n","\n","# # Save the image directly using the imsave function\n","# output_path = '/content/drive/MyDrive/segmentation_masks/59.png'\n","# plt.imsave(output_path, segmentation_labels_np, cmap=cmap)\n","\n","# # Close the figure to free up resources (optional)\n","# plt.close()\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# # Set the target size to 2592 x 1728\n","# target_h, target_w = 1728, 2592\n","# # Upsample the segmentation_labels tensor to match the target size\n","\n","# upsampled_segmentation_labels = F.interpolate(segmentation_labels.float(), size=(target_h, target_w), mode='nearest').long()\n","\n","# # Select the first channel and convert it to a NumPy array\n","# upsampled_segmentation_labels_np = upsampled_segmentation_labels[0].squeeze().cpu().numpy()\n","\n","# # Compute the number of classes directly from the tensor\n","# n_classes = upsampled_segmentation_labels.max().item() + 1\n","\n","# # Choose a suitable colormap for segmentation tasks (e.g., 'viridis' or 'jet')\n","# cmap = plt.get_cmap('viridis', n_classes)  # You can adjust the colormap as desired\n","\n","# # Create a new figure without displaying the plot\n","# plt.figure(figsize=(10, 10))  # You can adjust the figure size as needed\n","\n","# # Plot the segmentation labels as an image using the custom colormap\n","# plt.imshow(upsampled_segmentation_labels_np, cmap=cmap, vmin=0, vmax=n_classes - 1)\n","# plt.axis('off')  # Turn off axis labels and ticks\n","# plt.colorbar().remove()  # Remove the colorbar\n","\n","# # Save the image directly using the imsave function\n","# output_path = '/content/drive/MyDrive/segmentation_masks_upsampled/3.png'\n","# plt.imsave(output_path, upsampled_segmentation_labels_np, cmap=cmap)\n","\n","# # Close the figure to free up resources (optional)\n","# plt.close()\n","\n","# # Reset the print options to the default\n","# torch.set_printoptions(profile='default')\n","\n","\n","\n","\n","\n","\n","\n","n_classes = seg_probs.size(1)\n","vis_seg_probs = seg_probs.argmax(dim=1).float()/n_classes*255\n","vis_img = vis_seg_probs.sum(0, keepdim=True)\n","facer.show_bhw(vis_img)\n","facer.show_bchw(facer.draw_bchw(image, faces))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7MZa6IZ0pdC2","outputId":"0aa588f1-65eb-4901-b75b-6c2a368c7659"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Package(s) not found: facer\u001b[0m\u001b[33m\n","\u001b[0mNo faces detected in 1.jpg.\n","No faces detected in 12.jpg.\n","No faces detected in 0.jpg.\n","No faces detected in 2.jpg.\n"]}]}],"metadata":{"fileId":"2046b4a7-5b7a-4f90-baff-bf308e5d39e5","kernelspec":{"display_name":"Python 3.8.5 ('haya')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"5003f9c0e45db0f3c7a1cb5ce68068a67da34769148fd3aca0b7e6785b51f3d6"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}